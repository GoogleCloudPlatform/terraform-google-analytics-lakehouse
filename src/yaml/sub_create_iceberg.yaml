# Subworkflow to create BLMS and Iceberg tables
sub_create_iceberg:
    params: [temp_bucket_name, provisioner_bucket_name, dataproc_service_account_name, warehouse_bucket_name]
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse_ds
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_LOCATION")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch:
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris:
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig:
                    version: "1.1"
                    properties:
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": $${"gs://"+warehouse_bucket_name+"/warehouse"}
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.3_2.13:1.2.1"
                        "spark.dataproc.driverEnv.lakehouse_catalog": $${lakehouse_catalog}
                        "spark.dataproc.driverEnv.lakehouse_database": $${lakehouse_database}
                        "spark.dataproc.driverEnv.temp_bucket": $${temp_bucket_name}
                        "spark.dataproc.driverEnv.bq_dataset": $${bq_dataset}
                        "spark.dataproc.driverEnv.bq_gcs_connection": $${bq_gcs_connection}

                environmentConfig:
                    executionConfig:
                        serviceAccount: $${dataproc_service_account_name}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
        next: check_if_done

    # Poll job until completed
    - get_operation:
        call: http.get
        args:
            url: $${"https://dataproc.googleapis.com/v1/"+Operation.body.name}
            auth:
                type: OAuth2
        result: Operation

    - check_if_done:
        switch:
          - condition: $${"done" in Operation.body and Operation.body.done}
            next: returnOutput

    - wait:
        call: sys.sleep
        args:
            seconds: 10
        next: get_operation

    - returnOutput:
            return: Operation

