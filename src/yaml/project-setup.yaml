# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This defines the Google Workflow for the Analytics lakehouse Soultion: https://console.cloud.google.com/products/solutions/details/analytics-lakehouse
# This Workflow executes through Terraform. For Google Workflows executed via Terraform, variables are defined such that:
#
#     - Terraform environment variables are denoted by $
#     - Google Workflow variables are escaped via $$
#
# To modify this Workflow to stand alone (no Terraform):
#
#     - Replace vars in `main` -> `steps` -> `assign` with your own (or use https://cloud.google.com/workflows/docs/passing-runtime-arguments#gcloud)
#     - Change all $$ to $

main:
  params: []
  steps:
    # If this workflow has been run before, do not run again
    - sub_check_if_run:
        steps:
          - assign_values:
              assign:
                - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                - workflow_id: $${sys.get_env("GOOGLE_CLOUD_WORKFLOW_ID")}
          - get_executions:
              call: http.get
              args:
                url: $${"https://workflowexecutions.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/workflows/"+workflow_id+"/executions"}
                auth:
                  type: OAuth2
              result: Operation
          - check_if_run:
              switch:
                - condition: $${len(Operation.body.executions) > 1}
                  next: end
    - sub_wait_for_dataplex_discovery:
        steps:
          - assign_asset_ids:
              assign:
                - asset_ids:
                    - ${dataplex_asset_tables_id}
                    - ${dataplex_asset_textocr_id}
                    - ${dataplex_asset_ga4_id}
                    # ADD NEW ASSETS HERE
          - run_checks:
              parallel:
                for:
                  value: asset_id
                  in: $${asset_ids}
                  steps:
                    - run_check:
                        call: check_discovery_status
                        args:
                          asset_id: $${asset_id}
                        result: result
    # Extra wait after discovery for table publishing
    - sub_extra_dataplex_wait:
        call: sys.sleep
        args:
          seconds: 120
    - sub_create_tables:
        call: create_tables
        result: create_tables_output
    - sub_create_data_quality_rules:
        call: create_data_quality_rules
        result: create_data_quality_rules_output
    - sub_create_taxonomy:
        call: create_taxonomy
        result: create_taxonomy_output

# Subworkflow to check if Dataplex Discovery is complete
check_discovery_status:
  params: [asset_id]
  steps:
    - check_status:
        call: http.get
        args:
          url: $${"https://dataplex.googleapis.com/v1/"+asset_id}
          auth:
            type: OAuth2
        result: Asset
    - check_if_done:
        switch:
          - condition: $${Asset.body.state == "ACTIVE" and Asset.body.discoveryStatus.state == "SCHEDULED"}
            return: Asset
    - wait:
        call: sys.sleep
        args:
          seconds: 15
        next: check_status

# Subworkflow to create BigQuery views using stored procedures
create_tables:
  steps:
    - assignTables:
        assign:
          - lakehouse_catalog: ${lakehouse_catalog}
          - lakehouse_database: lakehouse_db
          - bq_dataset: ${bq_dataset}
          - results: {}
          - tables:
              create_view_ecommerce: $${"call gcp_lakehouse_ds.create_view_ecommerce()"}
              create_iceberg_tabes: $${"call gcp_lakehouse_ds.create_iceberg_tables('"+lakehouse_catalog+"', '"+lakehouse_database+"', '"+bq_dataset+"')"}
    - loopTables:
        for:
          value: key
          in: $${keys(tables)}
          steps:
            - runQuery:
                call: googleapis.bigquery.v2.jobs.query
                args:
                  projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                  body:
                    useLegacySql: false
                    useQueryCache: false
                    location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                    timeoutMs: 600000
                    query: $${tables[key]}
                result: queryResult
            - captureResults:
                assign:
                  - results[key]: $${queryResult}
    # Extra wait before proceeding to allow BQ jobs to finish
    - wait_for_bq_to_finish:
        call: sys.sleep
        args:
          seconds: 60
    - returnStep:
        return: $${results}

create_data_quality_rules:
  steps:
    - assign_values:
        assign:
          - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - create_data_quality_check:
        call: http.post
        args:
          url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataScans?dataScanId=orders"}
          auth:
            type: OAuth2
          body:
            displayName: orders
            data:
              resource: $${"//bigquery.googleapis.com/projects/"+project_id+"/datasets/gcp_primary_staging/tables/thelook_ecommerce_orders"}
            dataQualitySpec:
              rules:
                - column: order_id
                  dimension: COMPLETENESS
                  name: non-null
                  description: Sample rule for non-null column
                  threshold: 1.0
                  non_null_expectation: {}
                - column: user_id
                  dimension: COMPLETENESS
                  name: non-null
                  description: Sample rule for non-null column
                  threshold: 1.0
                  non_null_expectation: {}
                - column: created_at
                  description: Sample rule for non-null column
                  dimension: COMPLETENESS
                  name: non-null
                  threshold: 1.0
                  non_null_expectation: {}
                - column: order_id
                  dimension: UNIQUENESS
                  name: unique
                  description: Sample rule for unique column
                  uniqueness_expectation: {}
                - column: status
                  dimension: VALIDITY
                  name: one-of-set
                  description: Sample rule for values in a set
                  ignore_null: false
                  set_expectation:
                    values:
                    - Shipped
                    - Complete
                    - Processing
                    - Cancelled
                    - Returned
                - column: num_of_item
                  dimension: VALIDITY
                  name: range-values
                  description: Sample rule for values in a range
                  ignore_null: false
                  threshold: 0.99
                  range_expectation:
                    max_value: '1'
                    # min_value:
                    strict_max_enabled: false
                    strict_min_enabled: false
                - dimension: VALIDITY
                  name: not-empty-table
                  description: Sample rule for a non-empty table
                  tableConditionExpectation:
                    sqlExpression: COUNT(*) > 0
        result: Operation
    - returnResult:
        return: $${Operation}

# Subworkflow to Dataplex taxonomy
create_taxonomy:
  steps:
    - assign_values:
        assign:
          - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - ufdataplex_job:
        call: http.post
        args:
          url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataTaxonomies?alt=json&dataTaxonomyId=sample-taxonomy&validateOnly=False"}
          auth:
            type: OAuth2
          body:
            description: Sample Taxonomy Description
            displayName: Sample Taxonomy Display Name
        result: Operation
    - returnResult:
        return: $${Operation}
