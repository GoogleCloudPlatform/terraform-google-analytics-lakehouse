# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This defines the Google Workflow for the Analytics lakehouse Soultion: https://console.cloud.google.com/products/solutions/details/analytics-lakehouse
# This Workflow executes through Terraform. For Google Workflows executed via Terraform, variables are defined such that:
#
#     - Terraform environment variables are denoted by $
#     - Google Workflow variables are escaped via $$
#
# To modify this Workflow to stand alone (no Terraform):
#
#     - Replace vars in `main` -> `steps` -> `assign` with your own (or use https://cloud.google.com/workflows/docs/passing-runtime-arguments#gcloud)
#     - Change all $$ to $

main:
    params: []
    steps:
        - init:
            # Define local variables from terraform env variables
            assign:
                - temp_bucket_name: ${temp_bucket}
                - dataproc_service_account_name: ${dataproc_service_account}
                - provisioner_bucket_name: ${provisioner_bucket}
                - warehouse_bucket_name: ${warehouse_bucket}
        # TODO: change this to poll for BigQuery table creation
        - sub_wait_for_dataplex_discovery:
            call: sys.sleep
            args:
                seconds: 480
        - sub_create_tables:
            call: create_tables
            result: create_tables_output
        - sub_create_iceberg:
            call: create_iceberg
            args:
                temp_bucket_name: $${temp_bucket_name}
                dataproc_service_account_name: $${dataproc_service_account_name}
                provisioner_bucket_name: $${provisioner_bucket_name}
                warehouse_bucket_name: $${warehouse_bucket_name}
            result: create_iceberg_output
        - sub_create_taxonomy:
            call: create_taxonomy
            result: create_taxonomy_output

# Subworkflow to create BigQuery views
create_tables:
    steps:
        # Create and Assign Views
        - assignStepPolicies:
            assign:
                - results: {}
                # - marketing_user: ${marketing_user}
                # - data_analyst_user: ${data_analyst_user}
                - policy_map:
                    # create something that duplicates the table and adds the policy
                    # row_policy_usa_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY usa_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_users` GRANT TO ('serviceAccount:" + data_analyst_user + "')  FILTER USING (Country = 'United States')"}
                    # row_policy_product_category_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY product_category_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_products` GRANT TO ('serviceAccount:" + marketing_user + "') FILTER USING (Category = 'Swim' or Category = 'Active' or Category = 'Fashion Hoodies & Sweatshirts')"}
                    create_view_ecommerce: $${"call gcp_lakehouse_ds.create_view_ecommerce()"}
        - loopStepPolicies:
            for:
                value: key
                in: $${keys(policy_map)}
                steps:
                    - runQueryPolicies:
                        call: googleapis.bigquery.v2.jobs.query
                        args:
                            projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                            body:
                                useLegacySql: false
                                useQueryCache: false
                                location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                                timeoutMs: 600000
                                query: $${policy_map[key]}
                        result: queryResult
                    - sumStepPolicies:
                        assign:
                            - results[key]: $${queryResult}
        - returnStep:
            return: $${results}

# Subworkflow to create BLMS and Iceberg tables
create_iceberg:
    params: [temp_bucket_name, provisioner_bucket_name, dataproc_service_account_name, warehouse_bucket_name]
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse_ds
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_LOCATION")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch:
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris:
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig:
                    version: "1.1"
                    properties:
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": $${"gs://"+warehouse_bucket_name+"/warehouse"}
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.3_2.13:1.2.1"
                        "spark.dataproc.driverEnv.lakehouse_catalog": $${lakehouse_catalog}
                        "spark.dataproc.driverEnv.lakehouse_database": $${lakehouse_database}
                        "spark.dataproc.driverEnv.temp_bucket": $${temp_bucket_name}
                        "spark.dataproc.driverEnv.bq_dataset": $${bq_dataset}
                        "spark.dataproc.driverEnv.bq_gcs_connection": $${bq_gcs_connection}

                environmentConfig:
                    executionConfig:
                        serviceAccount: $${dataproc_service_account_name}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
        next: check_if_done

    # Poll job until completed
    - get_operation:
        call: http.get
        args:
            url: $${"https://dataproc.googleapis.com/v1/"+Operation.body.name}
            auth:
                type: OAuth2
        result: Operation

    - check_if_done:
        switch:
          - condition: $${"done" in Operation.body and Operation.body.done}
            next: returnOutput

    - wait:
        call: sys.sleep
        args:
            seconds: 10
        next: get_operation

    - returnOutput:
            return: Operation

# Subworkflow to Dataplex taxonomy
create_taxonomy:
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - ufdataplex_job:
        call: http.post
        args:
            url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataTaxonomies?alt=json&dataTaxonomyId=sample-taxonomy&validateOnly=False"}
            auth:
                type: OAuth2
            body:
              description: "Sample Taxonomy Description"
              displayName: "Sample Taxonomy Display Name"
        result: Operation
    - returnResult:
        return: $${Operation}

create_ml_model:
    steps:
    - runQueries:
                steps:
                - logTable:
                    call: sys.log
                    args:
                        text: $${"Building BQML Model"}
                - runQuery:
                    call: googleapis.bigquery.v2.jobs.query
                    args:
                        projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                        body:
                            useLegacySql: false
                            useQueryCache: false
                            location: 'us'
                            timeoutMs: 600000
                            query: "" #${"CREATE OR REPLACE MODEL `gcp_lakehouse_us_ds.census_model` OPTIONS ( model_type='LOGISTIC_REG', auto_class_weights=TRUE, input_label_cols=['income_bracket'] ) AS SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket FROM `bigquery-public-data.ml_datasets.census_adult_income`"}
                    result: queryResult
    - returnResults:
        return: $${queryResult}
