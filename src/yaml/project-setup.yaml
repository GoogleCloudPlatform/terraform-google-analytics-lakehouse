# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This defines the Google Workflow for the Analytics lakehouse Soultion: https://console.cloud.google.com/products/solutions/details/analytics-lakehouse
# This Workflow executes through Terraform. For Google Workflows executed via Terraform, variables are defined such that:
#
#     - Terraform environment variables are denoted by $
#     - Google Workflow variables are escaped via $$
#
# To modify this Workflow to stand alone (no Terraform):
#
#     - Replace vars in `main` -> `steps` -> `assign` with your own (or use https://cloud.google.com/workflows/docs/passing-runtime-arguments#gcloud)
#     - Change all $$ to $

main:
  params: []
  steps:
    # If this workflow has been run before, do not run again
    - sub_check_if_run:
        try:
          steps:
            - assign_values:
                assign:
                  - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                  - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                  - workflow_id: $${sys.get_env("GOOGLE_CLOUD_WORKFLOW_ID")}
            - get_executions:
                call: http.get
                args:
                  url: $${"https://workflowexecutions.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/workflows/"+workflow_id+"/executions"}
                  auth:
                    type: OAuth2
                result: Operation
            - check_if_run:
                switch:
                  - condition: $${len(Operation.body.executions) > 1}
                    next: end
        retry:
            predicate: $${retry_on_403_predicate}
            max_retries: 5
            backoff:
                initial_delay: 30
                max_delay: 120
                multiplier: 2.0
    - parallelStep:
        parallel:
          branches:
            - sub_enable_private_google_access:
                steps:
                  - call_enable_private_google_access:
                      call: enable_private_google_access
                      result: enable_private_google_access_output
            - sub_run_datascan_thelook:
                steps:
                  - call_create_datascan_thelook:
                      call: create_datascan
                      args:
                        id: thelook
                      result: create_datascan_output
            - sub_run_datascan_textocr_images:
                steps:
                  - call_create_datascan_textocr_images:
                      call: create_datascan
                      args:
                        id: textocr
                      result: create_datascan_output
            - sub_run_datascan_ga4_images:
                steps:
                  - call_create_datascan_ga4_images:
                      call: create_datascan
                      args:
                        id: ga4
                      result: create_datascan_output
            - sub_upload_spark_notebook:
                steps:
                  - call_upload_spark_notebook:
                      call: upload_notebook
                      args:
                        notebook_path: "spark_in_bigquery.ipynb"
                      result: upload_spark_notebook_output
            - sub_upload_bqml_notebook:
                steps:
                  - call_upload_bqml_notebook:
                      call: upload_notebook
                      args:
                        notebook_path: create_embeddings_with_bqml.ipynb
                      result: upload_bqml_notebook_output
            - sub_upload_sparkml_notebook:
                steps:
                  - call_upload_sparkml_notebook:
                      call: upload_notebook
                      args:
                        notebook_path: purchase_predictions_sparkml.ipynb
                      result: upload_sparkml_notebook_output

# Occasionally IAM permissions take time to propagate, so we retry if this happens.
retry_on_403_predicate:
    params: [error]
    steps:
        - check_error_code_is_403:
            switch:
              - condition: $${error.code == 403}
                return: true
        - not_retryable:
            return: false

retry_on_404_predicate:
    params: [error]
    steps:
        - check_error_code_is_404:
            switch:
              - condition: $${error.code == 404}
                return: true
        - not_retryable:
            return: false

enable_private_google_access:
  steps:
    - assign_values:
        assign:
          - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - configure_subnet:
        call: googleapis.compute.v1.subnetworks.setPrivateIpGoogleAccess
        args:
          project: $${project_id}
          region: $${location}
          subnetwork: default
          body:
            privateIpGoogleAccess: true
        result: enable_private_google_access_output

# Subworkflow to run datascans
run_datascan_job:
  params: [id]
  steps:
    - run_job: 
        try:
            call: http.post
            args:
                url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataScans/"+id+":run"}
                auth:
                    type: OAuth2
            result: run_datascan_output
        retry:
            predicate: $${retry_on_404_predicate}
            max_retries: 5
            backoff:
                initial_delay: 30
                max_delay: 120
                multiplier: 2.0

upload_notebook:
  params: [notebook_path]
  steps:
    - assign_values:
        assign:
          - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
          - repo_id: ${repo_id}
    - read_notebook_from_gcs:
        call: googleapis.storage.v1.objects.get
        args:
          bucket: ${notebooks_bucket}
          object: $${notebook_path}
          alt: MEDIA
        result: notebook_content
    - upload_notebook_to_dataform:
        call: http.post
        args:
          url: $${"https://dataform.googleapis.com/v1/"+repo_id+":commit"}
          auth:
            type: OAuth2
          body:
            commitMetadata:
              author:
                name: user
                emailAddress: user@thelook.com
              commit_message: Uploading notebook
            fileOperations:
              $${notebook_path}:
                writeFile:
                  contents: $${base64.encode(text.encode(notebook_content))}
        result: upload_notebook_output
