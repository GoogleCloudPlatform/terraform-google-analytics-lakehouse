# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This defines the Google Workflow for the Analytics lakehouse Soultion: https://console.cloud.google.com/products/solutions/details/analytics-lakehouse
# This Workflow executes through Terraform. For Google Workflows executed via Terraform, variables are defined such that:
#
#     - Terraform environment variables are denoted by $
#     - Google Workflow variables are escaped via $$
#
# To modify this Workflow to stand alone (no Terraform):
#
#     - Replace vars in `main` -> `steps` -> `assign` with your own (or use https://cloud.google.com/workflows/docs/passing-runtime-arguments#gcloud)
#     - Change all $$ to $

main:
  params: []
  steps:
    # If this workflow has been run before, do not run again
    - sub_check_if_run:
        steps:
          - assign_values:
              assign:
                - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                - workflow_id: $${sys.get_env("GOOGLE_CLOUD_WORKFLOW_ID")}
          - get_executions:
              call: http.get
              args:
                url: $${"https://workflowexecutions.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/workflows/"+workflow_id+"/executions"}
                auth:
                  type: OAuth2
              result: Operation
          - check_if_run:
              switch:
                - condition: $${len(Operation.body.executions) > 1}
                  next: end
    - sub_wait_for_dataplex_discovery:
        steps:
          - assign_asset_ids:
              assign:
                - asset_ids:
                    - ${dataplex_asset_tables_id}
                    - ${dataplex_asset_textocr_id}
                    - ${dataplex_asset_ga4_id}
                    # ADD NEW ASSETS HERE
          - run_checks:
              parallel:
                for:
                  value: asset_id
                  in: $${asset_ids}
                  steps:
                    - run_check:
                        call: check_discovery_status
                        args:
                          asset_id: $${asset_id}
                        result: result
    # Extra wait after discovery for table publishing
    - sub_extra_dataplex_wait:
        call: sys.sleep
        args:
          seconds: 120
    - sub_create_tables:
        call: create_tables
        result: create_tables_output
    - sub_create_taxonomy:
        call: create_taxonomy
        result: create_taxonomy_output

# Subworkflow to check if Dataplex Discovery is complete
check_discovery_status:
  params: [asset_id]
  steps:
    - check_status:
        call: http.get
        args:
          url: $${"https://dataplex.googleapis.com/v1/"+asset_id}
          auth:
            type: OAuth2
        result: Asset
    - check_if_done:
        switch:
          - condition: $${Asset.body.state == "ACTIVE" and Asset.body.discoveryStatus.state == "SCHEDULED"}
            return: Asset
    - wait:
        call: sys.sleep
        args:
          seconds: 15
        next: check_status

# Subworkflow to create BigQuery views using stored procedures
create_tables:
  steps:
    - assignTables:
        assign:
          - lakehouse_catalog: ${lakehouse_catalog}
          - lakehouse_database: lakehouse_db
          - bq_connection: ${bq_connection}
          - bq_dataset: ${bq_dataset}
          - results: {}
          - tables:
              create_view_ecommerce: $${"call gcp_lakehouse_ds.create_view_ecommerce()"}
              create_iceberg_tabes: $${'call gcp_lakehouse_ds.create_iceberg_tables('"+lakehouse_catalog+"', '"+lakehouse_database+"', '"+bq_connection+"', '"+bq_dataset+"')"}
    - loopTables:
        for:
          value: key
          in: $${keys(tables)}
          steps:
            - runQuery:
                call: googleapis.bigquery.v2.jobs.query
                args:
                  projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                  body:
                    useLegacySql: false
                    useQueryCache: false
                    location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                    timeoutMs: 600000
                    query: $${tables[key]}
                result: queryResult
            - captureResults:
                assign:
                  - results[key]: $${queryResult}
    - returnStep:
        return: $${results}

# Subworkflow to Dataplex taxonomy
create_taxonomy:
  steps:
    - assign_values:
        assign:
          - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - ufdataplex_job:
        call: http.post
        args:
          url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataTaxonomies?alt=json&dataTaxonomyId=sample-taxonomy&validateOnly=False"}
          auth:
            type: OAuth2
          body:
            description: Sample Taxonomy Description
            displayName: Sample Taxonomy Display Name
        result: Operation
    - returnResult:
        return: $${Operation}

create_ml_model:
  steps:
    - runQueries:
        steps:
          - logTable:
              call: sys.log
              args:
                text: $${"Building BQML Model"}
          - runQuery:
              call: googleapis.bigquery.v2.jobs.query
              args:
                projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                body:
                  useLegacySql: false
                  useQueryCache: false
                  location: us
                  timeoutMs: 600000
                  query: "" #${"CREATE OR REPLACE MODEL `gcp_lakehouse_us_ds.census_model` OPTIONS ( model_type='LOGISTIC_REG', auto_class_weights=TRUE, input_label_cols=['income_bracket'] ) AS SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket FROM `bigquery-public-data.ml_datasets.census_adult_income`"}
              result: queryResult
    - returnResults:
        return: $${queryResult}
