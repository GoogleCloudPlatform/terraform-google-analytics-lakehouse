# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This defines the Google Workflow for the Analytics lakehouse Soultion: https://console.cloud.google.com/products/solutions/details/analytics-lakehouse
# This Workflow executes through Terraform. For Google Workflows executed via Terraform, variables are defined such that:
#
#     - Terraform environment variables are denoted by $
#     - Google Workflow variables are escaped via $$
#
# To modify this Workflow to stand alone (no Terraform):
#
#     - Replace vars in `main` -> `steps` -> `assign` with your own (or use https://cloud.google.com/workflows/docs/passing-runtime-arguments#gcloud)
#     - Change all $$ to $

main:
    params: []
    steps:
        - init:
            # Define local variables from terraform env variables
            assign:
                - source_bucket_name: "data-analytics-demos-stage"
                - dest_images_bucket_name: ${images_bucket}
                - dest_tables_bucket_name: ${tables_bucket}
                - temp_bucket_name: ${temp_bucket}
                - dataproc_service_account_name: ${dataproc_service_account}
                - provisioner_bucket_name: ${provisioner_bucket}
        - sub_upgrade_assets:
            call: upgrade_assets
            result: upgrade_assets_output
        - sub_copy_data:
            parallel:
              branches:
                - copy_textocr_images:
                        call: copy_objects
                        args:
                            source_bucket_name: $${source_bucket_name}
                            prefix: "TextOCR"
                            dest_bucket_name: $${dest_images_bucket_name}
                        result: copy_textocr_images_output
                - copy_ga4_images:
                        call: copy_objects
                        args:
                            source_bucket_name: $${source_bucket_name}
                            prefix: "ga4"
                            dest_bucket_name: $${dest_images_bucket_name}
                        result: copy_ga4_output
                - copy_new_york_taxi_trips_tables:
                        call: copy_objects
                        args:
                            source_bucket_name: $${source_bucket_name}
                            prefix: "new-york"
                            dest_bucket_name: $${dest_tables_bucket_name}
                        result: copy_new_york_taxi_trips_tables_output
                - copy_thelook_ecommerce_tables:
                        call: copy_objects
                        args:
                            source_bucket_name: $${source_bucket_name}
                            prefix: "thelook"
                            dest_bucket_name: $${dest_tables_bucket_name}
                        result: copy_thelook_ecommerce_tables_output
        - sub_copy_tables:
            call: copy_objects
            args:
                source_bucket_name: $${source_bucket_name}
                prefix: "tables"
                dest_bucket_name: $${dest_tables_bucket_name}
            result: copy_tables_output
        # - sub_create_tables:
        #     call: create_tables
        #     result: create_tables_output
        - sub_create_iceberg:
            call: create_iceberg
            args:
                temp_bucket_name: $${temp_bucket_name}
                dataproc_service_account_name: $${dataproc_service_account_name}
                provisioner_bucket_name: $${provisioner_bucket_name}
            result: create_iceberg_output
        - sub_create_taxonomy:
            call: create_taxonomy
            result: create_taxonomy_output

# Subworkflow to upgrade all Dataplex Assets to Managed
# Subworkflow gets all lakes, then all zones within each lake, then all assets within each zone and upgrades
upgrade_assets:
    steps:
        - init:
            assign:
                - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                - zones: []
        - get_lakes:
            call: http.get
            args:
                url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/lakes"}
                auth:
                    type: OAuth2
            result: Response
        - assign_lakes:
            assign:
                - response_lakes: $${Response.body.lakes}
        - get_zones:
            for:
                value: lake
                index: i
                in: $${response_lakes}
                steps:
                  - get_zones_in_lake:
                      call: http.get
                      args:
                          url: $${"https://dataplex.googleapis.com/v1/"+lake.name+"/zones"}
                          auth:
                              type: OAuth2
                      result: Response
                  - assign_zones: 
                      assign:
                          - response_zones: $${Response.body.zones}
                  
                  - save_zones:
                        for:
                          value: zone
                          index: j
                          in: $${response_zones}
                          steps:
                              - save_to_list:
                                  assign:
                                    - zones: $${list.concat(zones, zone)}
        - get_and_upgrade_all_assets:
            for:
                value: zone
                index: i
                in: $${zones}
                steps:
                  - get_assets_in_zone:
                      call: http.get
                      args:
                          url: $${"https://dataplex.googleapis.com/v1/"+zone.name+"/assets"}
                          auth:
                              type: OAuth2
                      result: Response
                  - check_for_assets:
                      switch:
                        - condition: $${not("assets" in Response.body)}
                          next: continue
                  - assign_assets:
                      assign:
                          - response_assets: $${Response.body.assets}
                  - upgrade_all_assets:
                        for:
                            value: asset
                            index: j
                            in: $${response_assets}
                            steps:
                              - upgrade_asset:
                                  call: http.patch
                                  args:
                                      url: $${"https://dataplex.googleapis.com/v1/"+asset.name+"?updateMask=resourceSpec.readAccessMode"}
                                      auth:
                                          type: OAuth2
                                      body:
                                          resourceSpec:
                                              readAccessMode: "MANAGED"
    

# Subworkflow to copy initial objects
copy_objects:
    params: [source_bucket_name, prefix, dest_bucket_name]
    steps:
        - list_objects:
            call: googleapis.storage.v1.objects.list
            args:
                bucket: $${source_bucket_name}
                prefix: $${prefix}
            result: list_result
        - start_counter:
            assign:
                - copied_objects: 0
        - copy_objects:
                parallel:
                    shared: [copied_objects]
                    for:
                        value: object
                        index: i
                        in: $${list_result.items}
                        steps:
                            - copy:
                                try:
                                    steps:
                                        - copy_object:
                                            call: googleapis.storage.v1.objects.copy
                                            args:
                                                sourceBucket: $${source_bucket_name}
                                                sourceObject: $${text.url_encode(object.name)}
                                                destinationBucket: $${dest_bucket_name}
                                                destinationObject: $${text.url_encode(object.name)}
                                            result: copy_result
                                        - save_result:
                                            assign:
                                                - copied_objects: $${copied_objects + 1}
                                except:
                                    as: e
                                    raise:
                                        exception: $${e}
                                        sourceBucket: $${source_bucket_name}
                                        sourceObject: $${object.name}
                                        destinationBucket: $${dest_bucket_name}
        - finish:
            return: $${copied_objects + " objects copied"}

# Subworkflow to create BigLake tables
create_tables:
    steps:
        # Create and Assign Views
        - assignStepPolicies:
            assign:
                - results: {}
                # - marketing_user: ${marketing_user}
                # - data_analyst_user: ${data_analyst_user}
                - policy_map:
                    # row_policy_usa_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY usa_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_users` GRANT TO ('serviceAccount:" + data_analyst_user + "')  FILTER USING (Country = 'United States')"}
                    # row_policy_product_category_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY product_category_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_products` GRANT TO ('serviceAccount:" + marketing_user + "') FILTER USING (Category = 'Swim' or Category = 'Active' or Category = 'Fashion Hoodies & Sweatshirts')"}
                    create_view_ecommerce: $${"call gcp_lakehouse_ds.create_view_ecommerce()"}
        - loopStepPolicies:
            for:
                value: key
                in: $${keys(policy_map)}
                steps:
                    - runQueryPolicies:
                        call: googleapis.bigquery.v2.jobs.query
                        args:
                            projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                            body:
                                useLegacySql: false
                                useQueryCache: false
                                location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                                timeoutMs: 600000
                                query: $${policy_map[key]}
                        result: queryResult
                    - sumStepPolicies:
                        assign:
                            - results[key]: $${queryResult}
        - returnStep:
            return: $${results}

# Subworkflow to create BLMS and Iceberg tables
create_iceberg:
    params: [temp_bucket_name, provisioner_bucket_name, dataproc_service_account_name]
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - warehouse_bucket_name: "gs://${warehouse_bucket}/warehouse"
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse_ds
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_LOCATION")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch:
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris:
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig:
                    version: "1.1"
                    properties:
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": "$${warehouse_bucket_name}"
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                        "spark.dataproc.driverEnv.lakehouse_catalog": $${lakehouse_catalog}
                        "spark.dataproc.driverEnv.lakehouse_database": $${lakehouse_database}
                        "spark.dataproc.driverEnv.temp_bucket": $${temp_bucket_name}
                        "spark.dataproc.driverEnv.bq_dataset": $${bq_dataset}
                        "spark.dataproc.driverEnv.bq_gcs_connection": $${bq_gcs_connection}

                environmentConfig:
                    executionConfig:
                        serviceAccount: $${dataproc_service_account_name}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
        next: check_if_done

    # Poll job until completed
    - get_operation:
        call: http.get
        args:
            url: $${"https://dataproc.googleapis.com/v1/"+Operation.body.name}
            auth:
                type: OAuth2
        result: Operation

    - check_if_done:
        switch:
          - condition: $${"done" in Operation.body and Operation.body.done}
            next: returnOutput

    - wait:
        call: sys.sleep
        args:
            seconds: 10
        next: get_operation

    - returnOutput:
            return: Operation

# Subworkflow to Dataplex taxonomy
create_taxonomy:
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - ufdataplex_job:
        call: http.post
        args:
            url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataTaxonomies?alt=json&dataTaxonomyId=sample-taxonomy&validateOnly=False"}
            auth:
                type: OAuth2
            body:
              description: "Sample Taxonomy Description"
              displayName: "Sample Taxonomy Display Name"
        result: Operation
    - returnResult:
        return: $${Operation}

create_ml_model:
    steps:
    - runQueries:
                steps:
                - logTable:
                    call: sys.log
                    args:
                        text: $${"Building BQML Model"}
                - runQuery:
                    call: googleapis.bigquery.v2.jobs.query
                    args:
                        projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                        body:
                            useLegacySql: false
                            useQueryCache: false
                            location: 'us'
                            timeoutMs: 600000
                            query: "" #${"CREATE OR REPLACE MODEL `gcp_lakehouse_us_ds.census_model` OPTIONS ( model_type='LOGISTIC_REG', auto_class_weights=TRUE, input_label_cols=['income_bracket'] ) AS SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket FROM `bigquery-public-data.ml_datasets.census_adult_income`"}
                    result: queryResult
    - returnResults:
        return: $${queryResult}
