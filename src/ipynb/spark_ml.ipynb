{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML with Dataproc Serverless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook tutorial demonstrates the execution of Apache SparkML jobs using Dataproc Serverless. The example focuses on a typical machine learning pipeline, covering stages such as data ingestion and cleaning, feature engineering, modeling, and model evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "This tutorial guides you through the execution of an Apache SparkML job that involves fetching data from the lakehouse, conducting Exploratory Data Analysis (EDA), data cleaning, feature engineering, model training, model evaluation, result output, and saving the model to a Cloud Storage bucket.\n",
    "\n",
    "The tutorial leverages the following Google Cloud ML services:\n",
    "- `Dataproc`\n",
    "- `BigQuery`\n",
    "- `Vertex AI Training`\n",
    "\n",
    "Key steps include:\n",
    "\n",
    "- Setting up a Google Cloud project and Dataproc serverless.\n",
    "- Configuring the spark-bigquery-connector.\n",
    "- Ingesting data from the lakehouse into a Spark DataFrame.\n",
    "- Conducting Exploratory Data Analysis (EDA).\n",
    "- Visualizing data samples.\n",
    "- Cleaning the data.\n",
    "- Selecting features.\n",
    "- Training the model.\n",
    "- Outputting results.\n",
    "- Saving the model to a Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The [NYC TLC (Taxi and Limousine Commission) Trips](https://console.cloud.google.com/marketplace/product/city-of-new-york/nyc-tlc-trips) (New York taxi and limosine trips data) dataset is available in [BigQuery Public Datasets](https://cloud.google.com/bigquery/public-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial\n",
    "\n",
    "### Set your project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the current active project and store it as a list of strings.\n",
    "PROJECT_ID = !gcloud config get-value project\n",
    "\n",
    "# Extract the project ID from the list.\n",
    "PROJECT_ID = PROJECT_ID[0] if PROJECT_ID else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Cloud Storage bucket URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prefix of the bucket created via Terraform.\n",
    "BUCKET_PREFIX = \"gcp-lakehouse-model\"\n",
    "\n",
    "# Retrieve the Cloud Storage bucket URI for storing the machine learning model.\n",
    "BUCKET_URI = !gcloud storage buckets list --format='value(name)' --filter='name:{BUCKET_PREFIX}*'\n",
    "\n",
    "# Extract the bucket URI from the list.\n",
    "BUCKET_URI = BUCKET_URI[0] if BUCKET_URI else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopandas import gpd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "# A Spark Session is how you interact with Spark SQL to create Dataframes\n",
    "from pyspark.sql import SparkSession\n",
    "# PySpark functions\n",
    "from pyspark.sql.functions import col, floor, unix_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the SparkSession\n",
    "\n",
    "When setting up your SparkSession to work with Apache Spark and BigQuery, ensure that you include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VER = \"0.34.0\"\n",
    "FILE_NAME = f\"spark-bigquery-with-dependencies_2.12-{VER}.jar\"\n",
    "connector = f\"gs://spark-lib/bigquery/{FILE_NAME}\"\n",
    "\n",
    "# Set the log level to ERROR for the SparkSession to suppress the warning\n",
    "\n",
    "# Initialize the SparkSession.\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"spark-ml-taxi\")\n",
    "    .config(\"spark.jars\", connector)\n",
    "    .config(\"spark.logConf\", \"false\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data\n",
    "\n",
    "In this tutorial, you will be working with the NYC Taxi dataset, focusing on trips that occurred in 2022. Given the original dataset's substantial size of 36 million rows, completing this tutorial with the entire dataset would demand considerable time, resources, and associated costs. To address this, you will be working with a representative 5% sample of the data, allowing us to achieve the tutorial objectives more efficiently.\n",
    "\n",
    "Please note that utilizing a smaller sample size helps minimize the computational resources required and is a common practice for tutorial purposes. The concepts and techniques demonstrated with the sampled data remain applicable to the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NYC_taxi in Github Activity Public Dataset from BigQuery.\n",
    "taxi_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", f\"{PROJECT_ID}.gcp_primary_staging.new_york_taxi_trips_tlc_yellow_trips_2022\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Sample data to minimize the cost. \n",
    "# You can try different values in the parameter fraction, or remove the next line to use all.\n",
    "taxi_df = taxi_df.sample(fraction=0.05, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Exploratory Data Analysis (EDA)\n",
    "\n",
    "Initiate the Exploratory Data Analysis (EDA) phase to uncover patterns, relationships, and anomalies within the dataset. Employ various graphical and visualization techniques to gain insights into the data's characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove unnecessary columns and assess the null counts of the fields. Keep in mind that `pickup_location_id` and `dropoff_location_id` columns have been converted to strings but should be integers. To use these columns effectively, reformat these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose categorical columns to drop.\n",
    "columns_to_drop = [\n",
    "    'vendor_id',\n",
    "    'rate_code',\n",
    "    'store_and_fwd_flag',\n",
    "    'payment_type',\n",
    "    'data_file_year',\n",
    "    'data_file_month'\n",
    "]\n",
    "\n",
    "# Drop the specified columns and convert pickup_location_id and dropoff_location_id to integers.\n",
    "taxi_df = (\n",
    "    taxi_df\n",
    "    .drop(*columns_to_drop)\n",
    "    .withColumn(\"start_zone_id\", col(\"pickup_location_id\").cast(\"int\"))\n",
    "    .withColumn(\"end_zone_id\", col(\"dropoff_location_id\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "# Display summary statistics and preview the modified DataFrame.\n",
    "taxi_df.describe().show()\n",
    "taxi_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this summary, several key insights emerge:\n",
    "  * There are over 1 million trip histories for Yellow Taxi in 2022, which represents approximately 5% of the total trips.\n",
    "  * Certain trip histories contain abnormalies, such as very large numbers in `trip_distance`.\n",
    "  * The dataset's entries had included precise latitude and longitude data for both pickup and dropoff locations up until the year 2016. However, due to [privacy concerns](https://agkn.wordpress.com/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/), this granular location information was discontinued. Instead, the pickup_location_id and dropoff_location_id now align with the [NYC Taxi Zones](https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc), offering approximations based on the NYC Department of City Planningâ€™s Neighborhood Tabulation Areas (NTAs) to indicate approximate neighborhood locations.\n",
    "\n",
    "Based on the information above, the following filtering criteria can be used to remove unrealistic values from the dataset.\n",
    "  * Exclude entries with excessively large `trip_distance` values, surpassing 10,000 miles.\n",
    "  * Exclude entries with null and negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df = taxi_df.where(\n",
    "    (col(\"trip_distance\") < 10000) &\n",
    "    (col(\"fare_amount\") > 0) &\n",
    "    (col(\"extra\") >= 0) &\n",
    "    (col(\"mta_tax\") >= 0) &\n",
    "    (col(\"tip_amount\") >= 0) &\n",
    "    (col(\"tolls_amount\") >= 0) &\n",
    "    (col(\"imp_surcharge\") >= 0) &\n",
    "    (col(\"airport_fee\") >= 0) &\n",
    "    (col(\"total_amount\") > 0)\n",
    ").dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `unix_timestamp()` function to trasform `pickup_datetime` and `dropoff_datetime` into Unix Timestamp type.\n",
    "\n",
    "Once you have converted `pickup_datetime` and `dropoff_datetime` to `start_time` and `end_time` as Unix Timestamps, you can manipulate these timestamps to generate more compelling columnnar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the datetime from string to Unix timestamp.\n",
    "taxi_df = taxi_df.withColumn(\"start_time\", unix_timestamp(col(\"pickup_datetime\")))\n",
    "taxi_df = taxi_df.withColumn(\"end_time\", unix_timestamp(col(\"dropoff_datetime\")))\n",
    "\n",
    "# Determine if it's a weekday.\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"is_weekday\",\n",
    "    ((floor(col(\"start_time\") / 86400) + 4) % 7 > 0)\n",
    "    & ((floor(col(\"start_time\") / 86400) + 4) % 7 < 6),\n",
    ")\n",
    "\n",
    "# Convert start_time to start_time_in_hour.\n",
    "taxi_df = taxi_df.withColumn(\n",
    "    \"start_time_in_hour\", floor((col(\"start_time\") % 86400) / 60 / 60)\n",
    ")\n",
    "\n",
    "# Calculate trip_duration.\n",
    "taxi_df = taxi_df.withColumn(\"trip_duration\", col(\"end_time\") - col(\"start_time\"))\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "taxi_df = taxi_df.drop(\"pickup_datetime\", \"dropoff_datetime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Feature Engineering\n",
    "\n",
    "While the Taxi dataset contains trips for all NYC boroughs, the precise location information is categorized using `NYC Taxi zones`. To make the location ID with significance, acquire the GeoJSON format of NYC Taxi zones from the BigQuery public dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GeoJSON format of NYC Taxi zones from the BigQuery public dataset.\n",
    "geo_df = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"table\", \"bigquery-public-data.new_york_taxi_trips.taxi_zone_geom\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "# Convert Spark DataFrame into Pandas DataFrame to integrate with the GeoPandas library.\n",
    "geo_pd = geo_df.toPandas()\n",
    "\n",
    "# Create a GeoDataFrame based on the central point of each taxi zone, separated by latitude and longitude.\n",
    "geo_pd['long'] = gpd.GeoSeries.from_wkt(geo_pd['zone_geom']).centroid.x\n",
    "geo_pd['lat'] = gpd.GeoSeries.from_wkt(geo_pd['zone_geom']).centroid.y\n",
    "\n",
    "# Drop unnecessary columns.\n",
    "geo_pd = geo_pd[[\"zone_id\", \"long\", \"lat\"]]\n",
    "\n",
    "# Convert back to a Spark DataFrame.\n",
    "geo_spark_df = spark.createDataFrame(geo_pd)\n",
    "\n",
    "# Join taxi_df with geographic position for each start_zone_id.\n",
    "taxi_zone_df = (taxi_df\n",
    "        .join(geo_spark_df, taxi_df.start_zone_id == geo_spark_df.zone_id)\n",
    "        .withColumnRenamed(\"long\", \"start_long\")\n",
    "        .withColumnRenamed(\"lat\", \"start_lat\")\n",
    "        .drop(\"zone_id\")\n",
    "        .join(geo_spark_df, taxi_df.end_zone_id == geo_spark_df.zone_id)\n",
    "        .withColumnRenamed(\"long\", \"end_long\")\n",
    "        .withColumnRenamed(\"lat\", \"end_lat\")\n",
    "        .drop(\"zone_id\")\n",
    "       )\n",
    "\n",
    "taxi_zone_df.printSchema()\n",
    "taxi_zone_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `trip_distance`, `trip_duration` also has some extreme values. Remove unrealistic values from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter trips occurring between same taxi zones and exceeding where trip_duration is more than 28800 seconds (8 hours).\n",
    "taxi_df = taxi_zone_df.where(\n",
    "    (col(\"trip_duration\") < 28800) &\n",
    "    (col(\"start_zone_id\") != col(\"end_zone_id\"))\n",
    ")\n",
    "\n",
    "# Drop unnecessary columns\n",
    "taxi_df = taxi_df.drop(\"pickup_location_id\", \"dropoff_location_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the distributions for the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to be converted to numerical type.\n",
    "columns_to_convert = [\n",
    "    \"trip_distance\",\n",
    "    \"fare_amount\",\n",
    "    \"extra\",\n",
    "    \"mta_tax\",\n",
    "    \"tip_amount\",\n",
    "    \"tolls_amount\",\n",
    "    \"imp_surcharge\",\n",
    "    \"airport_fee\",\n",
    "    \"total_amount\"\n",
    "]\n",
    "\n",
    "# Convert Spark DataFrame into a Pandas DataFrame.\n",
    "taxi_pd = taxi_df.toPandas()\n",
    "\n",
    "# Convert columns of \"object\" type to the float type.\n",
    "taxi_pd[columns_to_convert] = taxi_pd[columns_to_convert].astype(float)\n",
    "\n",
    "# Display information about the Pandas DataFrame.\n",
    "taxi_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out outliers and organize the data into a box plot and histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns of interest for visualization.\n",
    "TAXI_COLUMNS_TO_SHOW = {\n",
    "    \"trip_distance\",\n",
    "    \"trip_duration\",\n",
    "    \"total_amount\",\n",
    "    \"fare_amount\",\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'imp_surcharge',\n",
    "    'airport_fee',\n",
    "}\n",
    "\n",
    "# Filter the DataFrame to include data within reasonable ranges.\n",
    "taxi_pd_filtered = taxi_pd.query(\n",
    "    \"trip_distance > 0 and trip_distance < 20 \\\n",
    "    and trip_duration > 0 and trip_duration < 10000\"\n",
    ")\n",
    "\n",
    "# Scatter plot to visualize the relationship between trip_distance and trip_duration.\n",
    "sns.relplot(\n",
    "    data=taxi_pd_filtered,\n",
    "    x=\"trip_distance\",\n",
    "    y=\"trip_duration\",\n",
    "    kind=\"scatter\",\n",
    ")\n",
    "\n",
    "# Disable the warning for too many open figures\n",
    "plt.rcParams['figure.max_open_warning'] = 0\n",
    "\n",
    "# Box plots and histograms for the specified columns.\n",
    "for column in taxi_pd_filtered.columns:\n",
    "    if column in TAXI_COLUMNS_TO_SHOW:\n",
    "        _, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "        taxi_pd_filtered[column].plot(kind=\"box\", ax=ax[0])\n",
    "        taxi_pd_filtered[column].plot(kind=\"hist\", ax=ax[1])\n",
    "        plt.title(column)\n",
    "        plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is right-skewed, and a positive correlation is observed between `trip_distance` and `trip_duration`. The majority of trips are completed within an hour (3600 sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "Not all features in our dataset will be utilized for training. After selecting the following columns as features, they must be assembled using `VectorAssembler()`, a feature transformer that consolidates multiple columns into a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected features for training the model.\n",
    "feature_cols = [\n",
    "    \"passenger_count\",\n",
    "    \"is_weekday\",\n",
    "    \"start_time_in_hour\",\n",
    "    \"trip_distance\",\n",
    "    \"start_time\",\n",
    "    \"end_time\",\n",
    "    \"is_weekday\",\n",
    "    \"start_time_in_hour\",\n",
    "    \"start_long\",\n",
    "    \"start_lat\",\n",
    "    \"end_long\",\n",
    "    \"end_lat\",\n",
    "    \"total_amount\",\n",
    "    \"fare_amount\",\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'imp_surcharge',\n",
    "    'airport_fee',\n",
    "]\n",
    "\n",
    "# Create a VectorAssembler with specified input and output columns.\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "\n",
    "# Transform each column into vector form using the VectorAssembler.\n",
    "taxi_transformed_data = assembler.transform(taxi_df)\n",
    "\n",
    "# Randomly split the transformed data into training and test sets.\n",
    "(taxi_training_data, taxi_test_data) = taxi_transformed_data.randomSplit([0.95, 0.05])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "PySpark offers various Regression Models, including  `LinearRegression`, `GeneralizedLinearRegression`, `DecisionTreeRegressor`, `RandomForestRegressor`, and `GBTRegressor`.\n",
    "\n",
    "`GBTRegressor`, known as `Gradient Boosted Trees` is a widely-used regression method employing ensembles of decision trees. This algorithm iteratively trains decision trees to minimize a loss function.\n",
    "Although this model is computationally expensive, empirical testing has demonstrated that `GBT Regressor` yields the most favorable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GBTRegressor model with specified input, output, and prediction columns.\n",
    "gbt = GBTRegressor(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"trip_duration\",\n",
    "    predictionCol=\"pred_trip_duration\",\n",
    ")\n",
    "\n",
    "# Define an evaluator for calculating the R2 score.\n",
    "evaluator_r2 = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"r2\"\n",
    ")\n",
    "\n",
    "# Define an evaluator for calculating the RMSE error.\n",
    "evaluator_rmse = RegressionEvaluator(\n",
    "    labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol(), metricName=\"rmse\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gradient Boosted Trees (GBT) model on the Taxi dataset. This process may take several minutes.\n",
    "taxi_gbt_model = gbt.fit(taxi_training_data)\n",
    "\n",
    "# Get predictions for the Taxi dataset using the trained GBT model.\n",
    "taxi_gbt_predictions = taxi_gbt_model.transform(taxi_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the R2 score for the Taxi dataset predictions.\n",
    "taxi_gbt_accuracy_r2 = evaluator_r2.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT R2 Accuracy = {taxi_gbt_accuracy_r2}\")\n",
    "\n",
    "# Evaluate the Root Mean Squared Error (RMSE) for the Taxi dataset predictions.\n",
    "taxi_gbt_accuracy_rmse = evaluator_rmse.evaluate(taxi_gbt_predictions)\n",
    "print(f\"Taxi Test GBT RMSE Accuracy = {taxi_gbt_accuracy_rmse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the result\n",
    "\n",
    "The trained model for the Taxi dataset yields an R2 score ranging betweein approximately 83-87% and a Root Mean Square Error(RMSE) of 200-300, the exact values dependent on the chose samples. In practical machine learning projects, cross-validation emerges as a crucial tool, enhancing data utilization. However, due to its iterative nature, cross-validation demands a substantial investment of time and computation resources to complete. For additional insights, refer to [Cross-validation (statistics)](https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model to a Cloud Storage path\n",
    "\n",
    "To ensure the preservation and accessibility of the trained model, it can be saved to a Cloud Storage path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a Cloud Storage path\n",
    "taxi_gbt_model.write().overwrite().save(f\"gs://{BUCKET_URI}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
