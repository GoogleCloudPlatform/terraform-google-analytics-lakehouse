# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Argument Formatting Notes:
# One $ denotes resources that will be formatted by terraform upon deployment
# Two $$ denotes resources that will be formatted at runtime
# The event payload is not used but contains the Pub/Sub message if required for future use

main:
    params: [event]
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - dataproc_service_account: ${dataproc_service_account}
            - provisioner_bucket_name: ${provisioner_bucket}
            - warehouse_bucket_name: "gs://${warehouse_bucket}/warehouse"
            - temp_bucket_name: $(temp_bucket}
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch:
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris:
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig:
                    version: "1.1"
                    properties:
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": "$${warehouse_bucket_name}"
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                        lakehouse_catalog: $${lakehouse_catalog}
                        lakehouse_database: $${lakehouse_database}
                        temp_bucket: $${temp_bucket_name}
                        bq_dataset: $${bq_dataset}
                        bq_gcs_connection: $${bq_gcs_connection}

                environmentConfig:
                    executionConfig:
                        serviceAccount: $${dataproc_service_account}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
    # TODO: add await result until Operation is complete

    - returnOutput:
            return: Operation
