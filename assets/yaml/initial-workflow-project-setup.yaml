# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

main:
    params: []
    steps:
        - sub_copy_objects:
            call: copy_objects
            result: output
        - sub_create_tables:
            call: create_tables
            result: output1
        - sub_create_iceberg:
            call: create_iceberg
            result: output2
        - sub_create_taxonomy:
            call: create_taxonomy
            result: output3

# Subworkflow to copy initial objects
copy_objects:
    steps:
        - init:
            assign:
                - source_bucket: "data-analytics-demos"
                - dest_bucket: ${raw_bucket}
                - copied_objects: []
        - list_objects:
            call: googleapis.storage.v1.objects.list
            args:
                bucket: $${source_bucket}
                prefix: "thelook"
            result: list_result
        - start_counter:
            assign:
                - copied_objects: 0
        - copy_objects:
                for:
                    value: object
                    index: i
                    in: $${list_result.items}
                    steps:
                      - step1:
                            try:
                                steps:
                                  - copy_object:
                                      call: googleapis.storage.v1.objects.copy
                                      args:
                                          sourceBucket: $${source_bucket}
                                          sourceObject: $${text.url_encode(object.name)}
                                          destinationBucket: $${dest_bucket}
                                          destinationObject: $${text.url_encode(object.name)}
                                      result: copy_result
                                  - save_result:
                                      assign:
                                          - copied_objects: $${copied_objects + 1}
                            except:
                                as: e
                                raise:
                                    exception: $${e}
                                    sourceBucket: $${source_bucket}
                                    sourceObject: $${object.name}
                                    destinationBucket: $${dest_bucket}
        - finish:
            return: $${copied_objects + " objects copied"}

# Subworkflow to create BigLake tables
create_tables:
    steps:
        - assignStepTables:
            assign:
                - results: {}
                - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                - bucket: ${raw_bucket}
                - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                - map:
                    gcp_tbl_order_items: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_order_items` WITH CONNECTION `"+location+".gcp_lakehouse_connection` OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/order_items-0*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
                    gcp_tbl_orders: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_orders` WITH CONNECTION `"+location+".gcp_lakehouse_connection`     OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/orders-*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
                    gcp_tbl_users: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_users` WITH CONNECTION `"+location+".gcp_lakehouse_connection` OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/users-*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
                    gcp_tbl_distribution_centers: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_distribution_centers` WITH CONNECTION `"+location+".gcp_lakehouse_connection` OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/distribution_centers-*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
                    gcp_tbl_inventory_items: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_inventory_items` WITH CONNECTION `"+location+".gcp_lakehouse_connection` OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/inventory_items-*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
                    gcp_tbl_products: $${"CREATE OR REPLACE EXTERNAL TABLE `gcp_lakehouse_ds.gcp_tbl_products` WITH CONNECTION `"+location+".gcp_lakehouse_connection` OPTIONS(format ='Parquet', uris = ['gs://" + bucket + "/thelook_ecommerce/products-0*.Parquet'], max_staleness = INTERVAL 30 MINUTE, metadata_cache_mode = 'AUTOMATIC');"}
        - loopStepTables:
            for:
                value: key
                in: $${keys(map)}
                steps:
                    - runQuery:
                        call: googleapis.bigquery.v2.jobs.query
                        args:
                            projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                            body:
                                useLegacySql: false
                                useQueryCache: false
                                location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                                timeoutMs: 600000
                                query: $${map[key]}
                        result: queryResult
                    - sumStep:
                        assign:
                            - results[key]: $${queryResult}
        # Create and Assign Views
        - assignStepPolicies:
            assign:
                - marketing_user: ${marketing_user}
                - data_analyst_user: ${data_analyst_user}
                - policy_map:
                    row_policy_usa_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY usa_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_users` GRANT TO ('serviceAccount:" + data_analyst_user + "')  FILTER USING (Country = 'United States')"}
                    row_policy_product_category_filter: $${"CREATE OR REPLACE ROW ACCESS POLICY product_category_filter ON `" + sys.get_env("GOOGLE_CLOUD_PROJECT_ID") + ".gcp_lakehouse_ds.gcp_tbl_products` GRANT TO ('serviceAccount:" + marketing_user + "') FILTER USING (Category = 'Swim' or Category = 'Active' or Category = 'Fashion Hoodies & Sweatshirts')"}
                    create_view_ecommerce: $${"call gcp_lakehouse_ds.create_view_ecommerce()"}
        - loopStepPolicies:
            for:
                value: key
                in: $${keys(policy_map)}
                steps:
                    - runQueryPolicies:
                        call: googleapis.bigquery.v2.jobs.query
                        args:
                            projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                            body:
                                useLegacySql: false
                                useQueryCache: false
                                location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
                                timeoutMs: 600000
                                query: $${policy_map[key]}
                        result: queryResult
                    - sumStepPolicies:
                        assign:
                            - results[key]: $${queryResult}
        - returnStep:
            return: $${results}

# Subworkflow to create BLMS and Iceberg tables
create_iceberg:
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - dataproc_service_account: ${dataproc_service_account}
            - provisioner_bucket_name: ${provisioner_bucket}
            - warehouse_bucket_name: "gs://${warehouse_bucket}/warehouse"
            - temp_bucket_name: ${temp_bucket}
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse_ds
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_LOCATION")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch:
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris:
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig:
                    version: "1.1"
                    properties:
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": "$${warehouse_bucket_name}"
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                        "spark.dataproc.driverEnv.lakehouse_catalog": $${lakehouse_catalog}
                        "spark.dataproc.driverEnv.lakehouse_database": $${lakehouse_database}
                        "spark.dataproc.driverEnv.temp_bucket": $${temp_bucket_name}
                        "spark.dataproc.driverEnv.bq_dataset": $${bq_dataset}
                        "spark.dataproc.driverEnv.bq_gcs_connection": $${bq_gcs_connection}

                environmentConfig:
                    executionConfig:
                        serviceAccount: $${dataproc_service_account}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
    # TODO: add await result until Operation is complete

    - returnOutput:
            return: Operation

# Subworkflow to Dataplex taxonomy
create_taxonomy:
    steps:
    - assign_values:
        assign:
            - project_id: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
            - location: $${sys.get_env("GOOGLE_CLOUD_LOCATION")}
    - ufdataplex_job:
        call: http.post
        args:
            url: $${"https://dataplex.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/dataTaxonomies?alt=json&dataTaxonomyId=sample-taxonomy&validateOnly=False"}
            auth:
                type: OAuth2
            body:
              description: "Sample Taxonomy Description"
              displayName: "Sample Taxonomy Display Name"
        result: Operation
    - returnResult:
        return: $${Operation}

create_ml_model:
    steps:
    - runQueries:
                steps:
                - logTable:
                    call: sys.log
                    args:
                        text: $${"Building BQML Model"}
                - runQuery:
                    call: googleapis.bigquery.v2.jobs.query
                    args:
                        projectId: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
                        body:
                            useLegacySql: false
                            useQueryCache: false
                            location: 'us'
                            timeoutMs: 600000
                            query: "" #${"CREATE OR REPLACE MODEL `gcp_lakehouse_us_ds.census_model` OPTIONS ( model_type='LOGISTIC_REG', auto_class_weights=TRUE, input_label_cols=['income_bracket'] ) AS SELECT age, workclass, marital_status, education_num, occupation, hours_per_week, income_bracket FROM `bigquery-public-data.ml_datasets.census_adult_income`"}
                    result: queryResult
    - returnResults:
        return: $${queryResult}
