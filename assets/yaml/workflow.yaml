# Argument Formatting Notes:
# One $ denotes resources that will be formatted by terraform upon deployment
# Two $$ denotes resources that will be formatted at runtime
# The event payload is not used but contains the Pub/Sub message if required for future use

main:
    params: [event]
    steps:
    - assign_values:
        assign:
            - project_id: sys.get_env("GOOGLE_CLOUD_PROJECT_ID")
            - location: sys.get_env("GOOGLE_CLOUD_LOCATION")
            - connection_name: bq_spark_connection
            - batch_name: $${"initial-setup-"+text.substring(sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),0,7)}
            - dataproc_service_account: ${dataproc_service_account}
            - provisioner_bucket_name: ${provisioner_bucket}
            - warehouse_bucket_name: ${warehouse_bucket}
            - temp_bucket_name: $(temp_bucket}
            - lakehouse_catalog: lakehouse_catalog
            - lakehouse_database: lakehouse_database
            - bq_dataset: gcp_lakehouse
            - bq_gcs_connection: $${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")+".gcp_gcs_connection"}
    - dataproc_serverless_job:
        call: http.post
        args:
            url: $${"https://dataproc.googleapis.com/v1/projects/"+project_id+"/locations/"+location+"/batches"}
            auth:
                type: OAuth2
            body:
                pysparkBatch: 
                    mainPythonFileUri: $${"gs://"+provisioner_bucket_name+"/bigquery.py"}
                    jarFileUris: 
                        - "gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar"
                        - "gs://spark-lib/biglake/iceberg-biglake-catalog-0.0.1-with-dependencies.jar"
                runtimeConfig: 
                    version: "1.0.29"
                    properties: 
                        "spark.sql.catalog.lakehouse_catalog": "org.apache.iceberg.spark.SparkCatalog"
                        "spark.sql.catalog.lakehouse_catalog.blms_catalog": "$${lakehouse_catalog}"
                        "spark.sql.catalog.lakehouse_catalog.catalog-impl": "org.apache.iceberg.gcp.biglake.BigLakeCatalog"
                        "spark.sql.catalog.lakehouse_catalog.gcp_location": "$${location}"
                        "spark.sql.catalog.lakehouse_catalog.gcp_project": "$${project_id}"
                        "spark.sql.catalog.lakehouse_catalog.warehouse": $${"gs://"+warehouse_bucket_name+"/warehouse"}
                        "spark.jars.packages": "org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:0.14.1"
                        lakehouse_catalog: $${lakehouse_catalog}
                        lakehouse_database: $${lakehouse_database}
                        temp_bucket: $${temp_bucket_name}
                        bq_dataset: $${bq_dataset}
                        bq_gcs_connection: $${bq_gcs_connection}
                
                environmentConfig: 
                    executionConfig:
                        serviceAccount: $${dataproc_service_account}
                        subnetworkUri: "dataproc-subnet"
            query:
                batchId: $${batch_name}
            timeout: 300
        result: Operation
    # TODO: add await result until Operation is complete


    - returnOutput:
            return: Operation